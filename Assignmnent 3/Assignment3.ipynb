{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate change sucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffObjectId', 'Country', 'ISO2', 'ISO3', 'F1961', 'F1962', 'F1963', 'F1964', 'F1965', 'F1966', 'F1967', 'F1968', 'F1969', 'F1970', 'F1971', 'F1972', 'F1973', 'F1974', 'F1975', 'F1976', 'F1977', 'F1978', 'F1979', 'F1980', 'F1981', 'F1982', 'F1983', 'F1984', 'F1985', 'F1986', 'F1987', 'F1988', 'F1989', 'F1990', 'F1991', 'F1992', 'F1993', 'F1994', 'F1995', 'F1996', 'F1997', 'F1998', 'F1999', 'F2000', 'F2001', 'F2002', 'F2003', 'F2004', 'F2005', 'F2006', 'F2007', 'F2008', 'F2009', 'F2010', 'F2011', 'F2012', 'F2013', 'F2014', 'F2015', 'F2016', 'F2017', 'F2018', 'F2019', 'F2020', 'F2021', 'F2022']\n",
      "['1', 'Afghanistan, Islamic Rep. of', 'AF', 'AFG', '-0.113', '-0.164', '0.847', '-0.764', '-0.244', '0.226', '-0.371', '-0.423', '-0.539', '0.813', '0.619', '-1.124', '0.232', '-0.489', '-0.445', '-0.286', '0.513', '0.129', '0.361', '0.6', '0.483', '-0.346', '0.164', '0.145', '0.283', '-0.141', '0.391', '0.919', '-0.205', '0.73', '-0.168', '-0.294', '0.22', '0.43', '0.359', '-0.116', '0.471', '0.675', '1.198', '0.993', '1.311', '1.365', '0.587', '1.373', '0.401', '1.72', '0.675', '0.704', '0.895', '1.613', '1.397', '0.223', '1.281', '0.456', '1.093', '1.555', '1.54', '1.544', '0.91', '0.498', '1.327', '2.012']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file = open('./Annual_Surface_Temperature_Change.csv')\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "rows = []\n",
    "for row in csvreader:\n",
    "    rows.append(row)\n",
    "    \n",
    "print(rows[0])\n",
    "print(rows[1])\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(y_pred, y_train)\n\u001b[1;32m     45\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mHybridNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m quantum_layer(params, x)\n\u001b[0;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the quantum layer\n",
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_layer(params, x):\n",
    "    qml.templates.AngleEmbedding(x, wires=range(n_qubits))\n",
    "    qml.templates.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "# Define the classical layers\n",
    "class HybridNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 4)\n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = quantum_layer(params, x)\n",
    "        x = torch.tensor(x, requires_grad=True)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Create the hybrid model\n",
    "model = HybridNN()\n",
    "params = torch.randn(3, n_qubits, 3, requires_grad=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training data and labels\n",
    "X_train = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[0.9], [0.1]], dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = nn.MSELoss()(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value is: 2.297633646576633\n",
      "The actual value is: 2.012\n",
      "\n",
      "The predicted value is: 1.5266683277177329\n",
      "The actual value is: 1.518\n",
      "\n",
      "The predicted value is: 2.1298173428083347\n",
      "The actual value is: 1.688\n",
      "\n",
      "The predicted value is: 1.3628067231817163\n",
      "The actual value is: 1.256\n",
      "\n",
      "The predicted value is: 3.0457948388310916\n",
      "The actual value is: 3.243\n",
      "\n",
      "The predicted value is: 1.4458902360714445\n",
      "The actual value is: 1.212\n",
      "\n",
      "The predicted value is: 1.052067590622572\n",
      "The actual value is: 0.839\n",
      "\n",
      "The predicted value is: 0.9923055423722509\n",
      "The actual value is: 0.77\n",
      "\n",
      "The predicted value is: 0.7741224542033097\n",
      "The actual value is: 0.643\n",
      "\n",
      "The predicted value is: 2.1311116079136445\n",
      "The actual value is: 1.707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "# Define a quantum device\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "# Define a quantum circuit using Pennylane\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(x):\n",
    "    qml.RY(x[0], wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Define a Pennylane hybrid model that combines the quantum circuit and PyTorch model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128).to(torch.float64)\n",
    "        self.fc2 = nn.Linear(128, 64).to(torch.float64)\n",
    "        self.fc3 = nn.Linear(64, 1).to(torch.float64)\n",
    "        self.quantum_layer = quantum_circuit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = torch.relu(self.fc1(x))\n",
    "        x2 = torch.relu(self.fc2(x1))\n",
    "        output = self.fc3(x2)\n",
    "\n",
    "        # Pass input through the quantum layer\n",
    "        quantum_output = self.quantum_layer(x)\n",
    "        output = output + quantum_output\n",
    "\n",
    "        return output\n",
    "\n",
    "# # Define a regression model\n",
    "# class RegressionModel(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(RegressionModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 128)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, 1)  # Output layer with 1 neuron for regression\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         output = self.fc3(x)\n",
    "#         return output\n",
    "\n",
    "# Instantiate the regression model\n",
    "input_size = 61\n",
    "model = HybridModel(input_size)\n",
    "\n",
    "# Define a loss function (Mean Squared Error) and an optimizer (e.g., Adam)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "file = open('./Annual_Surface_Temperature_Change.csv')\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "prev_years = []\n",
    "final_years = []\n",
    "first = True\n",
    "for row in csvreader:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "        \n",
    "    if row[0] == '225':\n",
    "        prev_years.append([float(val) if val != \"\" else 0.0 for val in row[4:len(row)-1]])\n",
    "        final_years.append([float(row[len(row)-1]) if row[len(row)-1] != \"\" else 0.0])\n",
    "        break\n",
    "\n",
    "    \n",
    "    prev_years.append([float(val) if val != \"\" else 0.0 for val in row[4:len(row)-1]])\n",
    "    final_years.append([float(row[len(row)-1]) if row[len(row)-1] != \"\" else 0.0])\n",
    "\n",
    "    \n",
    "file.close()\n",
    "\n",
    "prev_years = torch.tensor(prev_years, dtype=torch.float64)\n",
    "final_years = torch.tensor(final_years, dtype=torch.float64)\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(prev_years)\n",
    "    loss = criterion(output, final_years)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training, you can use the model to make predictions\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        test_input = prev_years[i]\n",
    "        predicted_output = model(test_input)\n",
    "\n",
    "    print(f\"The predicted value is: {predicted_output.item()}\")\n",
    "    print(f\"The actual value is: {final_years[i][0]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
